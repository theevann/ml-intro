{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Un exemple simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition de la fonction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser un réseau de neurone pour simuler la fonction:\n",
    "\n",
    "$f(x)=\\begin{cases}\n",
    "    0, & \\text{si $\\, 0 \\leq x \\leq 0.5$}\\\\\n",
    "    1, & \\text{si $\\, 0.5 < x \\leq 1$}\n",
    "  \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "y = f(x)\n",
    "\n",
    "print(\"x    :\", x)\n",
    "print(\"f(x) :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit un dataset de taille 1000 contenant des nombres aléatoires entre 0 et 1.  \n",
    "\n",
    "Ensuite, on divise ce dataset en 2 parties:\n",
    "- 80% seront des exemples d'entrainement (utilisés pour entraîner notre réseau)\n",
    "- 20% seront des exemples de validation (utilisés pour tester la qualité de notre réseau)\n",
    "\n",
    "Il est important de tester notre réseau sur des exemples qu'il n'a pas vu lors de son entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 1000\n",
    "train_size = dataset_size * 8 // 10\n",
    "\n",
    "numbers = torch.rand(dataset_size)  # On créé 1000 nombres aléatoires entre 0 et 1\n",
    "target = f(numbers).long()  # On applique la fonction f à ces nombres\n",
    "\n",
    "train_dataset = TensorDataset(numbers[:train_size], target[:train_size])\n",
    "val_dataset = TensorDataset(numbers[train_size:], target[train_size:])\n",
    "\n",
    "print(train_dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition du modèle du réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit un réseau de neurones avec **une couche cachée de taille 5**.  \n",
    "On utilise la non-linéarité ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 5),  # On met 1 car en entrée il n'y a qu'un seul nombre x\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 2),  # On met 2 car en sortie on prédit 2 classes 0 ou 1\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement du réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour entrainer notre modèle, il faut définir:\n",
    "- Un \"optimiser\" : C'est lui qui va **mettre à jour les paramètres du modèle** (en utilisant la méthode de descente de gradient).\n",
    "- Une fonction de loss : C'est la fonction qui **mesure la qualité de la prédiction** de notre réseau.  \n",
    "Plus la valeur de cette fonction est basse, plus la prediction est bonne. C'est donc cette fonction qu'on va minimiser avec notre optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10  # Nombre d'exemples d'entrainement entre chaque mise à jour des paramètres\n",
    "nb_epochs = 2   # Nombre de fois qu'on voit tous les exemples d'entrainement\n",
    "lr = .5  # \"Learning Rate\" : valeur qui multiplie le gradient dans la méthode de \"descente de gradient\"\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size)  # Le Dataloader sert à créer des groupes d'exemples d'entrainement (appelés batchs)\n",
    "optimizer = optim.SGD(model.parameters(), lr)  # L'optimiser qu'on utilise s'appelle SGD : Stochastic Gradient Descent\n",
    "loss_function = nn.NLLLoss()  # Cette fonction de loss calcule le log de la probabilité \n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.view(-1, 1))\n",
    "        probabilities = outputs.softmax(1) # Le softmax transforme la sortie du modèle en probabilités qui somment à 1\n",
    "        loss = loss_function(probabilities, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre réseau est maintenant entrainé !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test du réseau (Quantitatif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste notre modèle en calculant la précision de la prédiction sur notre ensemble de validation.  \n",
    "\n",
    "La précision est définit comme :  $\\text{précision} = \\dfrac{\\text{Nombre d'exemples correctement classifiés}}{\\text{Nombre d'exemples total}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(val_dataset, batch_size)  # On utilise le dataset de validation\n",
    "\n",
    "acc = 0\n",
    "for inputs, targets in dataloader:\n",
    "    outputs = model(inputs.view(-1, 1))\n",
    "    predictions = outputs.argmax(1)\n",
    "    acc += (predictions == targets).sum()\n",
    "\n",
    "acc = acc.float() / len(dataloader.dataset)\n",
    "print(\"Précision: {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test du réseau (Qualitatif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testez votre réseau !\n",
    "\n",
    "number = 0.45\n",
    "output = model(torch.Tensor([[number]]))\n",
    "print(output.softmax(1))  # Le softmax transforme la sortie du modèle en probabilités dont la somme fait 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va afficher la **fonction de décision** que le réseau a appris  :\n",
    "- En bleue, la fonction qui associe à chaque $x$ la probabilité que $x$ est au dessus de 0.5 \n",
    "- En orange, la décision prise : si la probabilité est inférieure à 0.5, on prédit 0, sinon on prédit 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.linspace(0,1)  # On créé une liste de nombres qui vont de 0 à 1 de facon régulière\n",
    "    y = model(x.unsqueeze(1)).softmax(1)[:, 1]  # On calcule la probabilité (courbe bleue) avec le réseau\n",
    "    decision = y.round()  # On calcule la décision (courbe orange) en arrondissant la probabilité\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(x.numpy(), y.numpy(), color='#1f77b4')\n",
    "plt.plot(x.numpy(), decision.numpy(), color='#ff7f0e');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Améliorer les résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour améliorer les résultats, essayer d'augmenter le nombre d'époques.  \n",
    "Par exemple, changer de 2 à 10 puis relancer la fonction précedente pour voir quelle fonction le réseau a appris.\n",
    "\n",
    "Dans ce cas, cela devrait suffire. En règle générale, on peut essayer de changer de nombreux paramètres:\n",
    "- Learning Rate\n",
    "- Batch Size\n",
    "- Paramètres de l'optimiser\n",
    "- Modèles (Mettre plus de neurones, plus de couches, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regardez les poids appris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Un autre exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A vous d'essayer !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous allons essayer de simuler la fonction:\n",
    "\n",
    "$f(x)=\\begin{cases}\n",
    "    1, & \\text{si $ \\, x < 0.1$ ou $ \\, 0.5 < x < 0.8$}\\\\\n",
    "    0, & \\text{sinon}\n",
    "  \\end{cases}$\n",
    "  \n",
    "Ci-dessous, vous pouvez utiliser:\n",
    "- le symbole `&` pour comme condition \"et\"\n",
    "- le symbole `|` comme condition \"ou\"\n",
    "\n",
    "Par exemple: ```(x > 0) & (x < 1)```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol_f\n",
    "\n",
    "def f(x):\n",
    "    return # Ecrire ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit un dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 10000\n",
    "train_size = dataset_size * 8 // 10\n",
    "\n",
    "numbers = torch.rand(dataset_size)  # On créé 1000 nombres aléatoires entre 0 et 1\n",
    "target = f(numbers).long()  # On applique la fonction f à ces nombres\n",
    "\n",
    "train_dataset = TensorDataset(numbers[:train_size], target[:train_size])\n",
    "val_dataset = TensorDataset(numbers[train_size:], target[train_size:])\n",
    "\n",
    "print(train_dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit le modèle :  \n",
    "$\\Longrightarrow$ Contruisez un modèle avec 2 couches cachées de 10 et 15 neurones respectivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load sol_model_2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # Ecrire ici\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraine le réseau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "nb_epochs = 10\n",
    "lr = 0.5\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size)\n",
    "optimizer = optim.SGD(model.parameters(), lr)\n",
    "loss_function = nn.CrossEntropyLoss()  # Cette fonction de loss combine le softmax et le calcul du log de la probabilité \n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.view(-1, 1))\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste le réseau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(val_dataset, batch_size)\n",
    "\n",
    "acc = 0\n",
    "for inputs, targets in dataloader:\n",
    "    outputs = model(inputs.view(-1, 1))\n",
    "    predictions = outputs.argmax(1)\n",
    "    acc += (predictions == targets).sum()\n",
    "\n",
    "acc = acc.float() / len(dataloader.dataset)\n",
    "print(\"Précision: {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regarde la fonction de décision :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.linspace(0,1)\n",
    "    y = model(x.unsqueeze(1)).softmax(1)[:, 1]\n",
    "    decision = y.round()\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(x.numpy(), y.numpy(), color='#1f77b4')\n",
    "plt.plot(x.numpy(), decision.numpy(), color='#ff7f0e');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Un réseau utile : Classification d'images de nombre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser un réseau de neurones pour résoudre un problème de classification d'images de chiffres manuscrits.  \n",
    "Le dataset que nous allons utiliser s'appelle MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# MNIST Dataset (Images and Labels)\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "val_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "print(\"Taille du dataset d'entrainement : %d images\" % len(train_dataset))\n",
    "print(\"Taille du dataset de validation : %d images\" % len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regardons des exemples d'images:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,9))\n",
    "for i in range(0, 5):\n",
    "    data, label = val_dataset[i]\n",
    "    data = data.squeeze()\n",
    "    fig.add_subplot(1, 5, i+1)\n",
    "    plt.imshow(data, cmap=\"gray\")\n",
    "    plt.xlabel(label, fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combien de classes le dataset contient-il ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input(\"Nombre de classes :\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Si on choisir au hasard, quelle est la probabilité qu'on choisisse la bonne classe ? Comment vérifier cette hypothèse ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input(\"Probabilité de choisir la bonne classe au hasard :\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit un modèle de réseau de neurones pour classifier les images.\n",
    "- En entrée, on a des images noir et blanc de taille 28x28 = 784 pixels.  \n",
    "On va \"simplement\" transformer chaque image en un grand vecteur de 784 nombres, on a donc **784 valeurs** en entrée.\n",
    "- On va utiliser 2 couches cachées de **75 et 50 neurones** respectivement.\n",
    "- En sortie, on veut la probabilité pour chaque classe, il faut donc **autant de neurones que de classes** en sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol_model_mnist\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # Ecrire ici\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du réseau avant entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,9))\n",
    "image_indices = torch.randint(0, len(val_dataset), (5,))  # On choisit 5 indices au hasard\n",
    "for i in range(0, 5):\n",
    "    j = image_indices[i].item()\n",
    "    data, true_label = val_dataset[j]\n",
    "    inputs = data.view(1, 784)  # On transforme la matrice 28x28 en vecteur de taille 784\n",
    "    outputs = model(inputs)\n",
    "    predicted_label = outputs.argmax(1).item()\n",
    "    \n",
    "    fig.add_subplot(1, 5, i+1)\n",
    "    plt.imshow(data.squeeze(), cmap=\"gray\")\n",
    "    plt.xlabel(predicted_label, fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, une seule époque suffit pour avoir des résultats corrects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "nb_epochs = 1\n",
    "lr = 0.5\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    print('Epoque %d' % epoch)\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.view(batch_size, 28*28) # On transforme la matrice 28x28 en vecteur de taille 784\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "print(\"-- Fin --\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(val_dataset, batch_size)\n",
    "\n",
    "acc = 0\n",
    "for inputs, targets in dataloader:\n",
    "    inputs = inputs.view(-1, 28*28)\n",
    "    outputs = model(inputs)\n",
    "    predictions = outputs.argmax(1)\n",
    "    acc += (predictions == targets).sum()\n",
    "\n",
    "acc = acc.float() / len(dataloader.dataset)\n",
    "print(\"Précision: {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du réseau après entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,9))\n",
    "image_indices = torch.randint(0, len(val_dataset), (5,))\n",
    "for i in range(0, 5):\n",
    "    j = image_indices[i].item()\n",
    "    data, true_label = val_dataset[j]\n",
    "    inputs = data.view(1, 784)\n",
    "    outputs = model(inputs)\n",
    "    predicted_label = outputs.argmax(1).item()\n",
    "    data = data.squeeze()\n",
    "    fig.add_subplot(1, 5, i+1)\n",
    "    plt.imshow(data, cmap=\"gray\")\n",
    "    plt.xlabel(predicted_label, fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# >>> [Cliquez ici pour aller à la suite du TP](Using-a-pretrained-CNN.ipynb) <<<"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
